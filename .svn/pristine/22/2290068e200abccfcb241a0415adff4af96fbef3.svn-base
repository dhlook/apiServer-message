package com.demo.apiserver.message.consumer;

import java.io.IOException;
import java.util.ArrayList;
import java.util.Collections;
import java.util.List;
import java.util.Properties;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;

import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.stereotype.Component;

import com.demo.apiserver.message.KafkaSettings;
import com.demo.apiserver.message.common.CryptoUtils;
import com.demo.apiserver.message.domain.BaseDomain;
import com.demo.apiserver.message.service.TopicService;
import com.fasterxml.jackson.databind.ObjectMapper;

@Component
public class MainConsumerGroup {
	private static final Logger logger = LoggerFactory.getLogger(MainConsumerGroup.class);
    // ObjectMapper는 thread-safe 하므로 static하게 사용한다.
    private static final ObjectMapper mapper = new ObjectMapper();

    private final KafkaConsumer<String, String> consumer;
    private final String topic;
    private TopicService service;
    private ExecutorService executor;
    private int defaultBatchSize;
    private List<BaseDomain> list = new ArrayList<BaseDomain>();

    public MainConsumerGroup() {
        this.consumer = null;
        this.topic = null;
    }

    public MainConsumerGroup(KafkaSettings kafkaSettings, String topic, TopicService service, int defaultBatchSize) {
        consumer = new KafkaConsumer<>(createConsumerConfig(kafkaSettings));

        this.topic = topic;
        this.service = service;
        this.defaultBatchSize = defaultBatchSize;
    }

    public void run(int numThreads) {
    	
    	executor = Executors.newFixedThreadPool(numThreads);

    	executor.submit(new Runnable() {
    		public void run() {
				try {
					consumer.subscribe(Collections.singletonList(topic));
		             
	             	while (true) {
	         			ConsumerRecords<String, String> consumerRecords = consumer.poll(1000);
	         			
	         			consumerRecords.forEach(record -> {
	         
		 					BaseDomain domain = null;
		         				
							try {
								domain = mapper.readValue(record.value(), BaseDomain.class);
							} catch (IOException e) {
								logger.error(e.getMessage());
							}
							 
							String payload = null;
							try {
								payload = CryptoUtils.decrypt(domain.getPayload());
								domain.setPayload(payload);
								 
								list.add(domain);
								if (list.size() >= defaultBatchSize) {
									service.messageListJob(topic, list);
									list.clear();
								}
							   
							} catch (Exception e) {
								logger.error("himLog decrypt error : " + e.getMessage());
							}
	     
	 					});
	             	}
				} catch(Exception e) {
					logger.info("MainConsumerGroup Run Exception : " + e.getMessage());
				} finally {
					consumer.close();
				}
    		}	
    	});
    }

    private static Properties createConsumerConfig(KafkaSettings kafkaSettings) {
    	Properties props = new Properties();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaSettings.getZookeeperConnect());
        props.put(ConsumerConfig.GROUP_ID_CONFIG, kafkaSettings.getGroupId());
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest"); // 초기오프셋 설정 - 마지막부터(latest), 처음부터(earliest), none
        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "true");    //오프셋 오토커밋 설정
        props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, kafkaSettings.getAutoCommitIntervalMs()); //오프셋 오토커밋 시간 
        props.put(ConsumerConfig.FETCH_MAX_BYTES_CONFIG, kafkaSettings.getFetchMessageMaxBytes()); // 한번에 가져올 수 있는 최대 데이터 사이즈
        return props;
    }

}
